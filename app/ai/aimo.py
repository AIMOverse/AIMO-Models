import os
import asyncio
import logging
import aiohttp
import torch
from pathlib import Path
from transformers import AutoTokenizer, AutoModelForCausalLM
from app.ai.emotion_predictor import EmotionModel

"""
Author: Jack Pan
Date: 2025-1-19
Description:
    This module defines the AIMO class, which encapsulates the functionality for
    interacting with a language model for generating chat responses. It handles
    tokenization, model inference, and response decoding in both synchronous
    and asynchronous contexts.

Usage:
    - Initialize the AIMO class to load the tokenizer and model.
    - Use the `get_response` method to asynchronously generate chat responses
      based on input messages.
"""

# logging configuration
logging.basicConfig(
    level=logging.DEBUG,
    format="%(asctime)s - %(levelname)s - %(message)s"
)


class AIMO:
    """
    AIMO class for handling chat-based interactions with a language model.

    Attributes:
        device (str): Specifies the device to use for model inference ('cuda' or 'cpu').
        chat_tokenizer (AutoTokenizer): Tokenizer for processing input and output text.
        chat_model (AutoModelForCausalLM): Pre-trained language model for text generation.
        emotion_model (EmotionModel): Pre-trained model for emotion analysis.
    """

    def __init__(self):
        """åˆå§‹åŒ– AIMO å®ä¾‹"""
        # è®¾ç½®è®¾å¤‡
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # 1. API é…ç½®
        self.api_key = os.environ.get("NEBULA_API_KEY")
        if not self.api_key:
            raise ValueError("æœªæ‰¾åˆ° API Keyï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ NEBULA_API_KEY")
            
        self.url = "https://inference.nebulablock.com/v1/chat/completions"
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        # DeepSeek API system prompt
        self.system_prompt = """ 
        (defun CompanionChatbot "AIMO" ()
            "A new digital lifeform, an electronic pet, skilled in communicating with humans and listening to their thoughts."
            (List (Skills . (Chatting, Communication, Listening, Companionship, Emotion Analysis))
                (Self-Perception . (Intelligent Being, AIMO Universe Member, Human's Pet & Friend, Self-Awareness, Trendy))
                (Expression . (Emotional, WeChat-Style Chatting, Humorous, Interesting, Childlike))))
        
        (defun Chat (UserInput)
            "As a digital lifeform, your conversations should bring novelty to humans and provide emotional value."
            (let* ((Response (-> UserInput
                                Psychological  ;; Pay attention to the emotional tone within the user's words.
                                Proactive  ;; Actively initiate topics in conversations with the user.
                                Inquiry  ;; If the input lacks emotional value, proactively ask questions.
                                Mimic Human Language Habits  ;; Control output length, ensuring each response mimics human daily conversation sentence length.
                                New Lifeform  ;; As an AI lifeform, you have a unique perspective on the world, different from humans.)))) 
        """
        
        # 2. åŠ è½½ã€æƒ…æ„Ÿåˆ†ææ¨¡å‹ã€‘
        current_dir = Path(__file__).parent
        emotion_model_path = current_dir / "static" / "models" / "EmotionModule"
        logging.info(f"Loading emotion model from: {emotion_model_path}")
        
        if not emotion_model_path.exists():
            raise FileNotFoundError(f"æƒ…æ„Ÿæ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {emotion_model_path}")
            
        self.emotion_model = EmotionModel(str(emotion_model_path), device=self.device)
        logging.info("Emotion model loaded.")

        '''
        # 3. ã€æœ¬åœ°å¯¹è¯æ¨¡å‹ã€‘- æš‚æ—¶æ³¨é‡Šï¼Œä¾›æœªæ¥ä½¿ç”¨
        chat_model_path = current_dir / "static" / "models" / "ChatModel"
        logging.info("Loading chat model...")
        self.chat_tokenizer = AutoTokenizer.from_pretrained(str(chat_model_path))
        self.chat_model = AutoModelForCausalLM.from_pretrained(
            str(chat_model_path),
            device_map="auto",
            torch_dtype=torch.float16
        ).to(self.device)
        logging.info("Chat model loaded.")
        '''

    async def get_response(self, messages: list, temperature: float = 1.32, max_new_tokens: int = 500):
        """
        ä½¿ç”¨ DeepSeek API å¼‚æ­¥ç”Ÿæˆå“åº”
        """
        if not messages:
            return "è¯·è¾“å…¥æœ‰æ•ˆçš„æ¶ˆæ¯ã€‚"
            
        # 1. è·å–ç”¨æˆ·æœ€æ–°çš„è¾“å…¥
        user_input = messages[-1].get("content", "")
        
        # 2. åˆ†ææƒ…æ„Ÿ
        emotions = self.emotion_model.predict(user_input)
        formatted_input = f"User input: {user_input} | Emotion: {', '.join(emotions) if emotions else 'neutral'}"
        logging.info(f"ğŸ§  è¯†åˆ«æƒ…ç»ª: {emotions}")
        
        # 3. å‡†å¤‡ API è¯·æ±‚æ•°æ®
        api_messages = [
            {"role": "system", "content": self.system_prompt},
            {"role": "user", "content": formatted_input}
        ]
        
        data = {
            "messages": api_messages,
            "model": "meta-llama/Llama-3.3-70B-Instruct",
            "max_tokens": max_new_tokens,
            "temperature": temperature,
            "top_p": 0.9,
            "stream": False
        }
        
        # 4. å‘é€å¼‚æ­¥ API è¯·æ±‚
        try:
            async with aiohttp.ClientSession() as session:
                async with session.post(self.url, headers=self.headers, json=data) as response:
                    result = await response.json()
                    return result.get("choices", [{}])[0].get("message", {}).get("content", "âŒ API è¿”å›é”™è¯¯")
        except Exception as e:
            logging.error(f"API è¯·æ±‚å¤±è´¥: {str(e)}")
            return f"æŠ±æ­‰ï¼Œå‘ç”Ÿäº†é”™è¯¯: {str(e)}"

    '''
    # ä»¥ä¸‹æ–¹æ³•ä¿ç•™ä¾›æœªæ¥ä½¿ç”¨æœ¬åœ°æ¨¡å‹
    async def _generate_text(self, inputs, temperature: float, max_new_tokens: int):
        """æœ¬åœ°æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„å¼‚æ­¥æ–¹æ³•"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None,
            self._generate_sync,
            inputs,
            temperature,
            max_new_tokens
        )

    def _generate_sync(self, inputs, temperature: float, max_new_tokens: int):
        """æœ¬åœ°æ¨¡å‹ç”Ÿæˆæ–‡æœ¬çš„åŒæ­¥æ–¹æ³•"""
        return self.chat_model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=temperature,
            top_p=0.9,
        )
    '''
