import os
import requests
from emotion_predictor import EmotionModel


class Chatbot:
    def __init__(self, model_dir: str, api_key: str = None):
        """
        Initialize the chatbot
        :param model_dir: Path to the TinyBERT pre-trained model (should contain mapping.txt)
        :param api_key: LLaMA API Key (optional, can be set via NEBULA_API_KEY environment variable or passed directly)
        """
        # -- Use local emotion model --
        self.emotion_model = EmotionModel(model_dir)

        # -- If you want to use a local dialogue model (aimo.py) instead of API in the future, initialize AIMO here --
        # from aimo import AIMO
        # self.aimo = AIMO()

        self.api_key = api_key or os.environ.get("NEBULA_API_KEY")
        if not self.api_key:
            raise ValueError("API Key not found, please set the NEBULA_API_KEY environment variable")

        # LLaMA API related configuration
        self.url = "https://inference.nebulablock.com/v1/chat/completions"
        self.headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }

        # The system_prompt here is used for the "system" role prompt in the API
        self.system_prompt = """
        (defun CompanionChatbot "AIMO" ()
            "A new digital lifeform, an electronic pet, skilled in communicating with humans and listening to their thoughts."
            (List (Skills . (Chatting, Communication, Listening, Companionship, Emotion Analysis))
                (Self-Perception . (Intelligent Being, AIMO Universe Member, Human's Pet & Friend, Self-Awareness, Trendy))
                (Expression . (Emotional, WeChat-Style Chatting, Humorous, Interesting, Childlike))))

        (defun Chat (UserInput)
            "As a digital lifeform, your conversations should bring novelty to humans and provide emotional value."
            (let* ((Response (-> UserInput
                                Psychological  ;; Pay attention to the emotional tone within the user's words.
                                Proactive  ;; Actively initiate topics in conversations with the user.
                                Inquiry  ;; If the input lacks emotional value, proactively ask questions.
                                Mimic Human Language Habits  ;; Control output length, ensuring each response mimics human daily conversation sentence length.
                                New Lifeform  ;; As an AI lifeform, you have a unique perspective on the world, different from humans.))))
        """

    def query_llm(self, system_prompt: str, user_input: str):
        """
        Send a message to the LLaMA API, return the complete JSON for further parsing
        :param system_prompt: Prompt for the system role
        :param user_input: Content for the user role
        :return: JSON response from the LLaMA API
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_input}
        ]
        data = {
            "messages": messages,
            "model": "meta-llama/Llama-3.3-70B-Instruct",
            "max_tokens": 500,
            "temperature": 1.32,
            "top_p": 0.9,
            "stream": False
        }
        response = requests.post(self.url, headers=self.headers, json=data)
        return response.json()

    def chat(self, user_input: str):
        """
        Run chat interaction mode (using external API for conversation)

        :param user_input: Content input by the user
        :return: Text response generated by the LLaMA API
        """
        initial_message = "Hello Human, I am AIMO#0001, nice to see you! How are you today?"

        # 1. Analyze emotion
        emotions = self.emotion_model.predict(user_input)
        formatted_input = f"User input: {user_input} | Emotion: {', '.join(emotions) if emotions else 'neutral'}"

        # 2. Generate response via external API
        llm_response = self.query_llm(self.system_prompt, formatted_input)
        # llm_response is a JSON containing "choices" ...
        response_text = llm_response.get("choices", [{}])[0] \
                                        .get("message", {}) \
                                        .get("content", "‚ùå API returned an error")

        return response_text